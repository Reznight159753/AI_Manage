import React, { Suspense, useEffect, useRef, useState, useMemo } from 'react';
import { useNavigate } from 'react-router-dom';
import { Canvas, useFrame } from '@react-three/fiber';
import { useGLTF, useTexture, Loader, Environment, useFBX, useAnimations, OrthographicCamera } from '@react-three/drei';
import { MeshStandardMaterial } from 'three/src/materials/MeshStandardMaterial';
import { LineBasicMaterial, MeshPhysicalMaterial, Vector2 } from 'three';
import ReactAudioPlayer from 'react-audio-player';
import createAnimation from '../converter';
import blinkData from '../blendDataBlink.json';
import * as THREE from 'three';
import axios from 'axios';
import { SRGBColorSpace, LinearSRGBColorSpace } from 'three';
import { Send, Loader2 } from "lucide-react";
import { Mic, MicOff } from "lucide-react";
import './ModelDesign.css';
const _ = require('lodash');

const host = 'http://localhost:5000'; // Server TTS

// AI Endpoints - s·ª≠ d·ª•ng proxy ƒë·ªÉ tr√°nh CORS
const AI_ENDPOINTS = [
  //'http://localhost:3001/api/ai',  // Proxy server
  'https://3a686927e9b9.ngrok-free.app/'      // Fallback: Direct connection
];

// Utility function ƒë·ªÉ t·∫°o UUID v4
function generateUUID() {
  return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
    var r = Math.random() * 16 | 0;
    var v = c == 'x' ? r : (r & 0x3 | 0x8);
    return v.toString(16);
  });
}

// H√†m l√†m s·∫°ch vƒÉn b·∫£n t·∫°i frontend - ch·ªâ lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát nguy hi·ªÉm
function cleanText(text) {
  return text
    .replace(/[\uD800-\uDFFF]/g, '') // Lo·∫°i b·ªè surrogate characters
    .replace(/[^\x20-\x7E\u00C0-\u1EF9\s.,!?\-()]/g, '') // Gi·ªØ ASCII, ti·∫øng Vi·ªát, d·∫•u c√¢u, d·∫•u c√°ch
    .trim();
}

function Avatar({ avatar_url, speak, setSpeak, text, setAudioSource, playing }) {
  let gltf = useGLTF(avatar_url);
  let morphTargetDictionaryBody = null;
  let morphTargetDictionaryLowerTeeth = null;

  const [
    bodyTexture, eyesTexture, teethTexture, bodySpecularTexture, bodyRoughnessTexture,
    bodyNormalTexture, teethNormalTexture, hairTexture, tshirtDiffuseTexture,
    tshirtNormalTexture, tshirtRoughnessTexture, hairAlphaTexture, hairNormalTexture,
    hairRoughnessTexture,
  ] = useTexture([
    "/images/body.webp", "/images/eyes.webp", "/images/teeth_diffuse.webp",
    "/images/body_specular.webp", "/images/body_roughness.webp", "/images/body_normal.webp",
    "/images/teeth_normal.webp", "/images/h_color.webp", "/images/tshirt_diffuse.webp",
    "/images/tshirt_normal.webp", "/images/tshirt_roughness.webp", "/images/h_alpha.webp",
    "/images/h_normal.webp", "/images/h_roughness.webp",
  ]);

  _.each([
    bodyTexture, eyesTexture, teethTexture, teethNormalTexture, bodySpecularTexture,
    bodyRoughnessTexture, bodyNormalTexture, tshirtDiffuseTexture, tshirtNormalTexture,
    tshirtRoughnessTexture, hairAlphaTexture, hairNormalTexture, hairRoughnessTexture,
  ], t => {
    t.colorSpace = SRGBColorSpace;
    t.flipY = false;
  });

  bodyNormalTexture.colorSpace = LinearSRGBColorSpace;
  tshirtNormalTexture.colorSpace = LinearSRGBColorSpace;
  teethNormalTexture.colorSpace = LinearSRGBColorSpace;
  hairNormalTexture.colorSpace = LinearSRGBColorSpace;

  gltf.scene.traverse(node => {
    if (node.type === 'Mesh' || node.type === 'LineSegments' || node.type === 'SkinnedMesh') {
      node.castShadow = true;
      node.receiveShadow = true;
      node.frustumCulled = false;

      if (node.name.includes("Body")) {
        node.material = new MeshPhysicalMaterial();
        node.material.map = bodyTexture;
        node.material.roughness = 1.7;
        node.material.roughnessMap = bodyRoughnessTexture;
        node.material.normalMap = bodyNormalTexture;
        node.material.normalScale = new Vector2(0.6, 0.6);
        node.material.color.setHex(0xF8E0A0);
        node.material.envMapIntensity = 0.8;
        morphTargetDictionaryBody = node.morphTargetDictionary;

        if (morphTargetDictionaryBody && morphTargetDictionaryBody['chinNarrow']) {
          node.morphTargetInfluences = node.morphTargetInfluences || [];
          node.morphTargetInfluences[morphTargetDictionaryBody['chinNarrow']] = 0.6;
        }
      }

      if (node.name.includes("Eyes")) {
        node.material = new MeshStandardMaterial();
        node.material.map = eyesTexture;
        node.material.roughness = 0.1;
        node.material.envMapIntensity = 0.5;
      }

      if (node.name.includes("Brows")) {
        node.material = new LineBasicMaterial({ color: 0x000000 });
        node.material.linewidth = 1;
        node.material.opacity = 0.5;
        node.material.transparent = true;
        node.visible = false;
      }

      if (node.name.includes("Teeth")) {
        node.material = new MeshStandardMaterial();
        node.material.roughness = 0.1;
        node.material.map = teethTexture;
        node.material.normalMap = teethNormalTexture;
        node.material.envMapIntensity = 0.7;
      }

      if (node.name.includes("Hair")) {
        node.material = new MeshStandardMaterial();
        node.material.map = hairTexture;
        node.material.alphaMap = hairAlphaTexture;
        node.material.normalMap = hairNormalTexture;
        node.material.roughnessMap = hairRoughnessTexture;
        node.material.transparent = true;
        node.material.depthWrite = false;
        node.material.side = 2;
        node.material.color.setHex(0x000000);
        node.material.envMapIntensity = 0.0;
      }

      if (node.name.includes("TSHIRT")) {
        node.material = new MeshStandardMaterial();
        node.material.map = tshirtDiffuseTexture;
        node.material.roughnessMap = tshirtRoughnessTexture;
        node.material.normalMap = tshirtNormalTexture;
        node.material.color.setHex(0xFFE4C4);
        node.material.envMapIntensity = 0.5;
      }

      if (node.name.includes("TeethLower")) {
        morphTargetDictionaryLowerTeeth = node.morphTargetDictionary;
      }
    }
  });

  const [clips, setClips] = useState([]);
  const mixer = useMemo(() => new THREE.AnimationMixer(gltf.scene), []);

  useEffect(() => {
    if (!speak) return;
    console.log('G·ª≠i vƒÉn b·∫£n t·ªõi server TTS:', text);
    makeSpeech(text)
      .then(response => {
        let { blendData, filename } = response.data;
        let newClips = [
          createAnimation(blendData, morphTargetDictionaryBody, 'HG_Body'),
          createAnimation(blendData, morphTargetDictionaryLowerTeeth, 'HG_TeethLower')
        ];
        filename = `${host}${filename}?t=${Date.now()}`;
        console.log('File MP3:', filename);
        setClips(newClips);
        setAudioSource(filename);
        setSpeak(false);
      })
      .catch(err => {
        console.error('L·ªói khi t·∫°o √¢m thanh:', err.message);
        setSpeak(false);
      });
  }, [speak, text, setAudioSource]);

  let idleFbx = useFBX('/idle.fbx');
  let { clips: idleClips } = useAnimations(idleFbx.animations);

  idleClips[0].tracks = _.filter(idleClips[0].tracks, track => {
    return track.name.includes("Head") || track.name.includes("Neck") || track.name.includes("Spine2");
  });

  idleClips[0].tracks = _.map(idleClips[0].tracks, track => {
    if (track.name.includes("Head")) track.name = "head.quaternion";
    if (track.name.includes("Neck")) track.name = "neck.quaternion";
    if (track.name.includes("Spine")) track.name = "spine2.quaternion";
    return track;
  });

  useEffect(() => {
    let idleClipAction = mixer.clipAction(idleClips[0]);
    idleClipAction.play();
    let blinkClip = createAnimation(blinkData, morphTargetDictionaryBody, 'HG_Body');
    let blinkAction = mixer.clipAction(blinkClip);
    blinkAction.play();
  }, [mixer]);

  useEffect(() => {
    if (!playing) return;
    _.each(clips, clip => {
      let clipAction = mixer.clipAction(clip);
      clipAction.setLoop(THREE.LoopOnce);
      clipAction.play();
    });
  }, [playing, clips]);

  useFrame((state, delta) => {
    mixer.update(delta);
  });

  return (
    <group name="avatar">
      <primitive object={gltf.scene} dispose={null} position={[-0.1, 0.5, 0]} scale={[0.6, 0.6, 0.6]} />
    </group>
  );
}

// H√†m g·ªçi API TTS (gi·ªØ nguy√™n)
function makeSpeech(text) {
  const cleanedText = cleanText(text);
  console.log('VƒÉn b·∫£n g·ª≠i ƒëi TTS:', cleanedText);
  if (!cleanedText) {
    console.error('L·ªói: VƒÉn b·∫£n sau khi l√†m s·∫°ch l√† r·ªóng');
    return Promise.reject(new Error('VƒÉn b·∫£n r·ªóng sau khi l√†m s·∫°ch'));
  }
  return axios.post(host + '/talk', { text: cleanedText, language: 'vi-VN', voice: 'vi-VN-HoaiMy' });
}

// H√†m g·ªçi API AI Assistant v·ªõi React proxy
async function callAIAssistant(userInput, sessionId) {
  const endpoints = [
    '/ask',  // React proxy
    'http://localhost:3001/api/ai/ask',  // Proxy server backup
    'https://3a686927e9b9.ngrok-free.app/ask'  // Direct connection backup
  ];
  
  for (const endpoint of endpoints) {
    try {
      const response = await axios.post(endpoint, {
        user_input: userInput,
        session_id: sessionId
      }, {
        timeout: 50000,
        headers: {
          'Content-Type': 'application/json',
          'Accept': 'application/json'
        },
        withCredentials: false
      });
      
      // Debug: Log response ƒë·ªÉ xem structure
      console.log('API Response:', response.data);
      
      const responseData = response.data;
      
      // X·ª≠ l√Ω response theo th·ª© t·ª± ∆∞u ti√™n
      // 1. N·∫øu response l√† string tr·ª±c ti·∫øp
      if (typeof responseData === 'string' && responseData.trim()) {
        return responseData.trim();
      }
      
      // 2. N·∫øu response c√≥ c√°c field th√¥ng th∆∞·ªùng
      if (responseData?.response && typeof responseData.response === 'string') {
        return responseData.response.trim();
      }
      
      if (responseData?.answer && typeof responseData.answer === 'string') {
        return responseData.answer.trim();
      }
      
      if (responseData?.message && typeof responseData.message === 'string') {
        return responseData.message.trim();
      }
      
      if (responseData?.data && typeof responseData.data === 'string') {
        return responseData.data.trim();
      }
      
      // 3. N·∫øu response l√† object, th·ª≠ convert th√†nh string
      if (typeof responseData === 'object' && responseData !== null) {
        // N·∫øu c√≥ keys, th·ª≠ l·∫•y value ƒë·∫ßu ti√™n l√† string
        const keys = Object.keys(responseData);
        for (const key of keys) {
          if (typeof responseData[key] === 'string' && responseData[key].trim()) {
            return responseData[key].trim();
          }
        }
        
        // Fallback: stringify object
        console.warn('Unknown response format:', responseData);
        return `Server response: ${JSON.stringify(responseData)}`;
      }
      
      // 4. Fallback cu·ªëi c√πng
      return 'Ph·∫£n h·ªìi kh√¥ng h·ª£p l·ªá t·ª´ server.';
      
    } catch (error) {
      console.error(`Error with endpoint ${endpoint}:`, {
        status: error.response?.status,
        data: error.response?.data,
        message: error.message
      });
      
      // N·∫øu l√† endpoint cu·ªëi c√πng v√† v·∫´n l·ªói
      if (endpoint === endpoints[endpoints.length - 1]) {
        return `L·ªói k·∫øt n·ªëi: ${error.message}`;
      }
      
      continue;
    }
  }
  
  return `Xin l·ªói, kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn server AI. Vui l√≤ng th·ª≠ l·∫°i sau.`;
}

// H√†m suggestedQuestions
// H√†m fetchSuggestedQuestions v·ªõi fallback endpoint
async function fetchSuggestedQuestions() {
  const endpoints = [
    '/get_unique_questions',  // React proxy
    'http://localhost:3001/api/ai/get_unique_questions',  // Proxy server backup
    'https://3a686927e9b9.ngrok-free.app/get_unique_questions'  // Direct connection backup
  ];

  const fallbackQuestions = [
    "tr∆∞·ªùng c√≥ bao nhi√™u khoa ?",
    "ƒë·ªëi t∆∞·ª£ng tuy·ªÉn sinh c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc",
    "ph·∫°m vi tuy·ªÉn sinh c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc",
    "hi"
  ];

  for (const endpoint of endpoints) {
    try {
      const response = await axios.get(endpoint, {
        timeout: 50000,
        headers: {
          'Content-Type': 'application/json',
          'Accept': 'application/json'
        },
        withCredentials: false
      });

      console.log("Suggested Questions Response:", response.data);

      const responseData = response.data;

      // ‚úÖ X·ª≠ l√Ω ƒë√∫ng format tr·∫£ v·ªÅ
      if (responseData?.unique_questions && Array.isArray(responseData.unique_questions)) {
        return responseData.unique_questions
          .map(item => (item.user ? item.user.trim() : null))
          .filter(q => q); // lo·∫°i b·ªè null/empty
      }

      console.warn("Unknown suggested questions response format:", responseData);
      return fallbackQuestions;

    } catch (error) {
      console.error(`Error with endpoint ${endpoint}:`, {
        status: error.response?.status,
        data: error.response?.data,
        message: error.message
      });

      if (endpoint === endpoints[endpoints.length - 1]) {
        return fallbackQuestions;
      }

      continue;
    }
  }

  return fallbackQuestions;
}

function Bg() {
  const texture = useTexture('/images/bg.webp');
  return (
    <mesh position={[0, 1.5, -2.5]}>
      <planeGeometry args={[1.4, 0.788]} />
      <meshBasicMaterial map={texture} />
    </mesh>
  );
}

function ModelDesign() {
  const navigate = useNavigate();
  const audioPlayer = useRef();
  const chatAreaRef = useRef();
  // State cho voice recording
  const [isRecording, setIsRecording] = useState(false);
  const [mediaRecorder, setMediaRecorder] = useState(null);
  const [audioChunks, setAudioChunks] = useState([]);
  
  // Session ID - t·∫°o UUID duy nh·∫•t cho m·ªói phi√™n
  const [sessionId] = useState(() => generateUUID());
  
  // States cho model 3D
  const [speak, setSpeak] = useState(false);
  const [text, setText] = useState("");
  const [speechText, setSpeechText] = useState("");
  const [audioSource, setAudioSource] = useState(null);
  const [playing, setPlaying] = useState(false);
  
  // State cho m√†n h√¨nh mobile
  const [isMobile, setIsMobile] = useState(false);
  
  // State cho hi·ªáu ·ª©ng typing animation
  const [displayedText, setDisplayedText] = useState("");
  const [isTyping, setIsTyping] = useState(false);
  const typingSpeed = 30; // T·ªëc ƒë·ªô typing c·ªë ƒë·ªãnh (ms)
  
  // State cho tr·∫°ng th√°i loading
  const [isProcessing, setIsProcessing] = useState(false);
  
  // State cho connection status
  const [connectionStatus, setConnectionStatus] = useState('checking');
  
  // States cho chat interface
  const [messages, setMessages] = useState([
    {
      id: 1,
      type: 'ai',
      content: 'Xin ch√†o! T√¥i l√† Arwen, tr·ª£ l√Ω AI c·ªßa b·∫°n. T√¥i c√≥ th·ªÉ tr√≤ chuy·ªán v√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa b·∫°n. H√£y nh·∫≠p tin nh·∫Øn ƒë·ªÉ b·∫Øt ƒë·∫ßu!',
      timestamp: new Date()
    }
  ]);

  // State cho suggested questions
  const [suggestedQuestions, setSuggestedQuestions] = useState([]);
  // const [showSuggestions, setShowSuggestions] = useState(true);

  const [currentTime, setCurrentTime] = useState(
    new Date().toLocaleTimeString('en-US', { 
      hour: '2-digit', 
      minute: '2-digit', 
      second: '2-digit', 
      hour12: true 
    }) + ' +07, ' + new Date().toLocaleDateString('en-GB')
  );

  // Test connection khi component mount
  useEffect(() => {
    const testConnection = async () => {
      console.log('üîç Testing connection to AI server...');
      try {
        const response = await callAIAssistant('test', sessionId);
        if (response.includes('kh√¥ng th·ªÉ k·∫øt n·ªëi')) {
          setConnectionStatus('failed');
        } else {
          setConnectionStatus('connected');
          console.log('‚úÖ Connection test successful');
        }
      } catch (error) {
        setConnectionStatus('failed');
        console.log('‚ùå Connection test failed:', error);
      }
    };
    
    testConnection();
  }, [sessionId]);

  useEffect(() => {
    console.log('üöÄ Component mounted, loading suggested questions...');
    
    const loadSuggestions = async () => {
      try {
        const questions = await fetchSuggestedQuestions();
        console.log('üìù Setting suggested questions:', questions);
        setSuggestedQuestions(questions.slice(0, 5)); // Ch·ªâ l·∫•y 5 c√¢u ƒë·∫ßu

        // Force log state ƒë·ªÉ debug
        setTimeout(() => {
          console.log('üìä Current suggestedQuestions state:', questions.slice(0, 5));
          console.log('üìä Current showSuggestions state:', true);
        }, 100);
        
      } catch (error) {
        console.error('‚ùå Error in loadSuggestions:', error);
        // V·∫´n set fallback questions
        setSuggestedQuestions([
          "tr∆∞·ªùng c√≥ bao nhi√™u khoa ?",
          "ƒë·ªëi t∆∞·ª£ng tuy·ªÉn sinh c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc", 
          "ph·∫°m vi tuy·ªÉn sinh c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc",
          "hi",
        ]);
      }
    };
    
    loadSuggestions();
  }, []);

  // Ki·ªÉm tra k√≠ch th∆∞·ªõc m√†n h√¨nh
  useEffect(() => {
    const checkScreenSize = () => {
      const isMobileScreen = window.innerWidth <= 768 || 
                           (window.innerWidth < window.innerHeight && window.innerWidth <= 1024);
      setIsMobile(isMobileScreen);
    };

    checkScreenSize();
    window.addEventListener('resize', checkScreenSize);
    return () => window.removeEventListener('resize', checkScreenSize);
  }, []);

  // C·∫≠p nh·∫≠t th·ªùi gian
  useEffect(() => {
    const timer = setInterval(() => {
      setCurrentTime(
        new Date().toLocaleTimeString('en-US', { 
          hour: '2-digit', 
          minute: '2-digit', 
          second: '2-digit', 
          hour12: true 
        }) + ' +07, ' + new Date().toLocaleDateString('en-GB')
      );
    }, 1000);
    return () => clearInterval(timer);
  }, []);

  // Typing animation effect
  useEffect(() => {
    if (!speechText || !isTyping) return;
    
    let index = 0;
    setDisplayedText("");
    
    const timer = setInterval(() => {
      setDisplayedText(speechText.slice(0, index + 1));
      index++;
      
      if (index >= speechText.length) {
        clearInterval(timer);
        setIsTyping(false);
      }
    }, typingSpeed);
    
    return () => clearInterval(timer);
  }, [speechText, isTyping]);

  // T·ª± ƒë·ªông scroll xu·ªëng cu·ªëi khi c√≥ tin nh·∫Øn m·ªõi
  useEffect(() => {
    if (chatAreaRef.current) {
      chatAreaRef.current.scrollTop = chatAreaRef.current.scrollHeight;
    }
  }, [messages]);

  // Debug: Log session ID v√† connection status
  useEffect(() => {
    console.log('Session ID:', sessionId);
    console.log('Connection Status:', connectionStatus);
  }, [sessionId, connectionStatus]);

  async function handleSuggestedQuestionClick(question) {
    if (isProcessing) return;
    
    const userMessage = {
      id: Date.now(),
      type: 'user',
      content: question,
      timestamp: new Date()
    };

    const newMessages = [...messages, userMessage];
    setMessages(newMessages);
    
    setIsProcessing(true);

    try {
      const aiResponse = await callAIAssistant(question, sessionId);
      
      const aiMessage = {
        id: Date.now() + 1,
        type: 'ai',
        content: aiResponse,
        timestamp: new Date()
      };
      
      const updatedMessages = [...newMessages, aiMessage];
      setMessages(updatedMessages);
      
      // K√≠ch ho·∫°t speech n·∫øu kh√¥ng ph·∫£i error message
      if (!aiResponse.includes('kh√¥ng th·ªÉ k·∫øt n·ªëi') && !aiResponse.includes('Ph·∫£n h·ªìi kh√¥ng h·ª£p l·ªá')) {
        setSpeechText(aiResponse);
        setIsTyping(true);
        setSpeak(true);
        setConnectionStatus('connected');
      } else {
        setConnectionStatus('failed');
      }
      
    } catch (error) {
      const errorMessage = {
        id: Date.now() + 1,
        type: 'ai',
        content: `ƒê√£ x·∫£y ra l·ªói: ${error.message}`,
        timestamp: new Date()
      };
      
      const updatedMessages = [...newMessages, errorMessage];
      setMessages(updatedMessages);
      setConnectionStatus('failed');
    }
    
    setIsProcessing(false);
  }

  // H√†m g·ªçi API Speech-to-Text
  async function callSpeechToText(audioBlob) {
    const formData = new FormData();
    formData.append('file', audioBlob, 'recording.wav');

    const endpoints = [
      'https://40cb57cb8ef5.ngrok-free.app/transcribe',
      '/transcribe', // N·∫øu c√≥ proxy
    ];

    for (const endpoint of endpoints) {
      try {
        console.log(`Calling Speech-to-Text: ${endpoint}`);
        
        const response = await axios.post(endpoint, formData, {
          headers: {
            'Content-Type': 'multipart/form-data',
          },
          timeout: 30000, // 30s cho upload audio
        });

        console.log('Speech-to-Text response:', response.data);
        return response.data.text || response.data.transcription || '';
        
      } catch (error) {
        console.error(`STT Error with ${endpoint}:`, error);
        continue;
      }
    }
    
    throw new Error('Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Speech-to-Text server');
  }
  // B·∫Øt ƒë·∫ßu recording
  async function startRecording() {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        } 
      });
      
      const recorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      const chunks = [];
      
      recorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunks.push(event.data);
        }
      };
      
      recorder.onstop = async () => {
        const audioBlob = new Blob(chunks, { type: 'audio/webm' });
        await processVoiceInput(audioBlob);
        
        // D·ª´ng t·∫•t c·∫£ tracks
        stream.getTracks().forEach(track => track.stop());
      };
      
      recorder.start();
      setMediaRecorder(recorder);
      setAudioChunks(chunks);
      setIsRecording(true);
      
      console.log('üé§ Started recording...');
      
    } catch (error) {
      console.error('‚ùå Error starting recording:', error);
      alert('Kh√¥ng th·ªÉ truy c·∫≠p microphone. Vui l√≤ng cho ph√©p quy·ªÅn truy c·∫≠p.');
    }
  }

  // D·ª´ng recording
  function stopRecording() {
    if (mediaRecorder && isRecording) {
      mediaRecorder.stop();
      setIsRecording(false);
      setMediaRecorder(null);
      console.log('‚èπÔ∏è Stopped recording');
    }
  }

  // X·ª≠ l√Ω voice input
  async function processVoiceInput(audioBlob) {
    setIsProcessing(true);
    
    try {
      // Convert to WAV if needed
      const wavBlob = await convertToWav(audioBlob);
      
      // G·ªçi Speech-to-Text API
      console.log('üîÑ Converting speech to text...');
      const transcribedText = await callSpeechToText(wavBlob);
      
      if (transcribedText.trim()) {
        // Set text v√†o input
        setText(transcribedText);
        
        // T·ª± ƒë·ªông g·ª≠i lu√¥n
        const userMessage = {
          id: Date.now(),
          type: 'user',
          content: transcribedText.trim(),
          timestamp: new Date()
        };

        const newMessages = [...messages, userMessage];
        setMessages(newMessages);

        // G·ªçi AI
        const aiResponse = await callAIAssistant(transcribedText, sessionId);
        
        const aiMessage = {
          id: Date.now() + 1,
          type: 'ai',
          content: aiResponse,
          timestamp: new Date()
        };
        
        setMessages([...newMessages, aiMessage]);
        
        if (!aiResponse.includes('kh√¥ng th·ªÉ k·∫øt n·ªëi')) {
          setSpeechText(aiResponse);
          setIsTyping(true);
          setSpeak(true);
          setConnectionStatus('connected');
        }
        
        setText(""); // Clear input
      } else {
        alert('Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c gi·ªçng n√≥i. Vui l√≤ng th·ª≠ l·∫°i.');
      }
      
    } catch (error) {
      console.error('‚ùå Voice processing error:', error);
      alert('L·ªói x·ª≠ l√Ω gi·ªçng n√≥i: ' + error.message);
    }
    
    setIsProcessing(false);
  }

// Convert audio to WAV format
  async function convertToWav(audioBlob) {
    // ƒê∆°n gi·∫£n h√≥a: return blob g·ªëc, server s·∫Ω handle conversion
    return audioBlob;
  }

// Audio player handlers
function playerEnded() {
    setAudioSource(null);
    setSpeak(false);
    setPlaying(false);
  }

  function playerReady() {
    audioPlayer.current.audioEl.current.play();
    setPlaying(true);
  }

  // Chat handlers
  function handleTextChange(e) {
    const value = e.target.value.substring(0, 500);
    //const cleanedText = cleanText(value);
    setText(value);
  }

  async function handleSend() {
    if (!text.trim() || isProcessing) return;

    const userMessage = {
      id: Date.now(),
      type: 'user',
      content: text.trim(),
      timestamp: new Date()
    };

    const newMessages = [...messages, userMessage];
    setMessages(newMessages);

    const currentInput = text.trim();
    setText("");
    setIsProcessing(true);

    try {
      const aiResponse = await callAIAssistant(currentInput, sessionId);
      
      const aiMessage = {
        id: Date.now() + 1,
        type: 'ai',
        content: aiResponse,
        timestamp: new Date()
      };
      
      const updatedMessages = [...newMessages, aiMessage];
      setMessages(updatedMessages);
      
      // K√≠ch ho·∫°t speech n·∫øu kh√¥ng ph·∫£i error message
      if (!aiResponse.includes('kh√¥ng th·ªÉ k·∫øt n·ªëi') && !aiResponse.includes('Ph·∫£n h·ªìi kh√¥ng h·ª£p l·ªá')) {
        setSpeechText(aiResponse);
        setIsTyping(true);
        setSpeak(true);
        setConnectionStatus('connected');
      } else {
        setConnectionStatus('failed');
      }
      
    } catch (error) {
      const errorMessage = {
        id: Date.now() + 1,
        type: 'ai',
        content: `ƒê√£ x·∫£y ra l·ªói: ${error.message}`,
        timestamp: new Date()
      };
      
      const updatedMessages = [...newMessages, errorMessage];
      setMessages(updatedMessages);
      setConnectionStatus('failed');
    }
    
    setIsProcessing(false);
  }

  function handleLogin() {
    navigate('/login');
  }

  function handleKeyPress(e) {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSend();
    }
  }

  function handleVoiceChat() {
    if (isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  }

  return (
    <div className={`container ${isMobile ? 'mobile-layout' : 'desktop-layout'}`}>
      {/* Header - hi·ªán tr√™n c√πng khi mobile */}
      {isMobile && (
        <div className="mobile-header">
          <div className="avatar-info">
            <div className="avatar-icon">A</div>
            <div className="avatar-name">
              Arwen AI 
              <span className={`connection-status ${connectionStatus}`}>
                {connectionStatus === 'connected' && 'üü¢'}
                {connectionStatus === 'failed' && 'üî¥'}
                {connectionStatus === 'checking' && 'üü°'}
              </span>
            </div>
          </div>
          <div className="header-right">
            <div className="current-time">{currentTime}</div>
            <button onClick={handleLogin} className="login-button">
              ƒêƒÉng nh·∫≠p
            </button>
          </div>
        </div>
      )}

      {/* Model 3D Panel */}
      <div className="model-panel">
        <Canvas dpr={2} onCreated={(ctx) => {
          ctx.gl.physicallyCorrectLights = true;
        }}>
          <OrthographicCamera
            makeDefault
            zoom={isMobile ? 800 : 1300}
            position={[0, 1.5, 1]}
          />

          <Suspense fallback={null}>
            <Environment background={false} files="/images/photo_studio_loft_hall_1k.hdr" />
          </Suspense>

          <Suspense fallback={null}>
            <Bg />
          </Suspense>

          <Suspense fallback={null}>
            <Avatar
              avatar_url="/model.glb"
              speak={speak}
              setSpeak={setSpeak}
              text={speechText}
              setAudioSource={setAudioSource}
              playing={playing}
            />
          </Suspense>
        </Canvas>
        <Loader dataInterpolation={(p) => `ƒêang t·∫£i... vui l√≤ng ƒë·ª£i`} />
        
        {/* Speech Bubble trong model */}
        {(displayedText || isTyping) && (
          <div className={`speech-bubble ${isMobile ? 'mobile-bubble' : ''}`}>
            <div className="bubble-text">
              {displayedText}
              {isTyping && <span className="typing-cursor">|</span>}
            </div>
            <div className="bubble-tail"></div>
          </div>
        )}
      </div>

      {/* Chat Panel */}
      <div className="chat-panel">
        {/* Header cho desktop */}
        {!isMobile && (
          <div className="header_model">
            <div className="avatar-info">
              <div className="avatar-icon">A</div>
              <div className="avatar-name">
                Arwen AI
                <span className={`connection-status ${connectionStatus}`}>
                  {connectionStatus === 'connected' && 'üü¢'}
                  {connectionStatus === 'failed' && 'üî¥'}
                  {connectionStatus === 'checking' && 'üü°'}
                </span>
              </div>
              {/* <div className="session-info">Session: {sessionId.substring(0, 8)}...</div> */}
              <div className="session-info">Session: {sessionId}</div>
            </div>
            <div className="header-right">
              <div className="current-time">{currentTime}</div>
              <button onClick={handleLogin} className="login-button">
                ƒêƒÉng nh·∫≠p
              </button>
            </div>
          </div>
        )}

        {/* Chat Messages - ch·ªâ hi·ªán tr√™n desktop */}
        {!isMobile && (
          <div className="chat-area" ref={chatAreaRef}>
            {/* Connection status message */}
            {connectionStatus === 'failed' && (
              <div className="message system-message">
                üî¥ L·ªói k·∫øt n·ªëi AI Server. ƒêang c·ªë g·∫Øng k·∫øt n·ªëi l·∫°i...
              </div>
            )}
            {connectionStatus === 'connected' && messages.length === 1 && (
              <div className="message system-message">
                üü¢ ƒê√£ k·∫øt n·ªëi th√†nh c√¥ng v·ªõi AI Server!
              </div>
            )}
            
            {messages.map((message) => (
              <div
                key={message.id}
                className={`message ${message.type === 'user' ? 'user-message' : 'ai-message'}`}
              >
                {message.content}
              </div>
            ))}        
            {/* Loading indicator */}
            {isProcessing && (
              <div className="message ai-message processing">
                <div className="typing-indicator">
                  <span></span>
                  <span></span>
                  <span></span>
                </div>
              </div>
            )}
            {!text.trim() && suggestedQuestions.length > 0 && (
              <div className="suggested-questions user-suggestions">
                <div className="suggestions-grid">
                  {suggestedQuestions.map((question, index) => (
                    <button
                      key={index}
                      className="suggestion-button"
                      onClick={() => handleSuggestedQuestionClick(question)}
                      disabled={isProcessing}
                    >
                      {question}
                    </button>
                  ))}
                </div>
              </div>
            )}
          </div>
        )}

        {/* Input Area */}
        <div className={`input-area ${isMobile ? 'mobile-only-input' : ''}`}>
          <div className="input-container">
            <textarea
              className="text-input"
              value={text}
              onChange={handleTextChange}
              onKeyPress={handleKeyPress}
              placeholder="Nh·∫≠p tin nh·∫Øn c·ªßa b·∫°n..."
              rows={1}
              disabled={isProcessing}
            />
            <div className="button-group">
              <button
                onClick={handleSend}
                className={`send-button ${(!text.trim() || isProcessing) ? 'disabled-button' : ''}`}
                disabled={!text.trim() || isProcessing}
              >
                {isProcessing ? <Loader2 className="animate-spin" size={16} /> : <Send size={16} />}
              </button>
              <button
                onClick={handleVoiceChat}
                className={`voice-button ${isRecording ? 'recording' : ''}`}
                // title={isRecording ? "D·ª´ng ghi √¢m" : "B·∫Øt ƒë·∫ßu ghi √¢m"}
                disabled={isProcessing}
              >
                {isRecording ? <MicOff size={16} /> : <Mic size={16} />}
              </button>
            </div>
          </div>
        </div>
      </div>

      <ReactAudioPlayer
        src={audioSource}
        ref={audioPlayer}
        onEnded={playerEnded}
        onCanPlayThrough={playerReady}
      />
    </div>
  );
}

export default ModelDesign;